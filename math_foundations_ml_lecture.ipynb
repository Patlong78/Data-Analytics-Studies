{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Mathematics Foundations for Machine Learning\n","### 1-Hour Lecture Notebook\n","---\n","This lecture covers:\n","- Linear Algebra (vectors, matrices, eigenvalues)\n","- Calculus (gradients, optimization)\n","- Probability & Statistics (distributions, Bayes theorem)\n","- Optimization methods (SGD, Adam, convexity)\n","\n","---"]},{"cell_type":"markdown","metadata":{},"source":["## Linear Algebra Basics\n","### Vectors and Matrices\n","A **vector** is a list of numbers:\n","$$ v = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{bmatrix} $$\n","\n","A **matrix** transforms vectors:\n","$$ A = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix} $$"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","v = np.array([1, 2])\n","A = np.array([[2, -1], [1, 3]])\n","Av = A @ v\n","\n","v, Av"]},{"cell_type":"markdown","metadata":{},"source":["### Eigenvalues and Eigenvectors\n","Solve:\n","$$ A x = \\lambda x $$"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np.linalg.eig(A)"]},{"cell_type":"markdown","metadata":{},"source":["## Calculus Foundations\n","### Gradients\n","The gradient of a function measures how it changes:\n","$$ f(x) = x^2 \\quad \\Rightarrow \\quad \\nabla f = 2x $$"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x = np.linspace(-3, 3, 200)\n","f = x**2\n","df = 2*x\n","\n","plt.plot(x, f)\n","plt.title('f(x) = x^2')\n","plt.xlabel('x')\n","plt.ylabel('f(x)')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Probability & Statistics\n","### Normal Distribution\n","$$ p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} $$"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mu, sigma = 0, 1\n","x = np.linspace(-4, 4, 300)\n","p = (1/np.sqrt(2*np.pi*sigma**2)) * np.exp(-(x-mu)**2/(2*sigma**2))\n","\n","plt.plot(x, p)\n","plt.title('Normal Distribution')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Optimization: SGD\n","### Stochastic Gradient Descent Update\n","$$ w := w - \\eta \\nabla_w L $$"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["eta = 0.1\n","w = 4\n","\n","trajectory = []\n","for _ in range(20):\n","    grad = 2*w\n","    w = w - eta * grad\n","    trajectory.append(w)\n","\n","plt.plot(trajectory)\n","plt.title('SGD Optimization Path for f(w)=w^2')\n","plt.xlabel('Iteration')\n","plt.ylabel('w')\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.6 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.6"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":5}
